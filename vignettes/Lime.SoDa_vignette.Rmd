---
title: "Lime.SoDa Vignette"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Lime.SoDa_vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
Precision Liming Soil Datasets (Lime.SoDa) is a collection of datasets from a field- and farm-scale soil mapping context. These datasets are 'ready-to-go' for modeling purposes, as they contain target soil properties and features in a tabular format. Target soil properties for all datasets are; soil organic matter (SOM) or -carbon (SOC), pH and clay, while the features for modeling are dataset specific. The goal of `Lime.SoDa` is to enable more reliable benchmarking and comparison of various modeling approaches in Digital Soil Mapping and Pedometrics by providing an open collection of multiple datasets. This vignette offers an overview of the `Lime.SoDa` package and how to use it for modelling purposes.

### Installation
Installing `Lime.SoDa` from GitHub by: 

```{r setup}
#remotes::install_github("JonasSchmidinger/Lime.SoDa") 

library(Lime.SoDa)
```

### Lime.SoDa package structure
`Lime.SoDa` contains 31 entries; 30 datasets and an overview table:

```{r}
data(package = "Lime.SoDa")$results[,3] # All entries in Lime.SoDa
```
`Overview_Datasets` provides a summary of the specific of the 30 datasets, including the dataset ID, number of samples, number of features and further information:


```{r}
head(Overview_Datasets)
```

Dataset IDs are named according to the geographic region and sample size. For example, `BB.250` refers to a dataset from Brandenburg (BB), Germany, which includes 250 reference soil samples. Additional information about the methodology, sampling procedures, feature codes, and more can be found in the metadata of each dataset which can be accessed through:

```{r}
?BB.250 #Access metadata
```

Datasets are stored as a list. This list contains the tabular dataset (`$Dataset`), pre-defined folds (`$Folds`) for a 10-fold cross-validation  and, if available, the spatial coordinates of the soil samples (`$Coordinates`):

```{r}
head(BB.250$Dataset)

BB.250$Folds

head(BB.250$Coordinates)
```

Each dataset has SOM or SOC, pH and Clay as target soil properties, which are always stored in the first three columns of the `$Dataset`. They can be recognized by their suffix `_target` e.g., `Clay_target`:

```{r}
str(BB.250$Dataset[1:3]) 
```

Features for modelling are dataset-specific and originate from laboratory-based spectroscopy, in-situ proximal soil sensing and remote sensing. They are stored in the columns of the `$Dataset` after the target soil properties:

```{r}
#Features in BB.250
colnames(BB.250$Dataset[-c(1:3)])
#Features in W.50
colnames(W.50$Dataset[-c(1:3)])
```

Spatial coordinates where either (1) included, (2) excluded or (3) dummy covariates were added. Reasons on why they were not included are specified in the metadata. For example, in `BB.250` coordinates are included and it can be accessed through `$Coordinates`. The EPSG code is provided in the column name. Conversely, in `SSP.58` coordinates were excluded, so that `$Coordinates` returns an `NA` value. In the case of `UL.120`, `$Coordinates` returns dummy coordinates which is highlighted by the column names:

```{r}
#When coordinates are included:
head(BB.250$Coordinates)

#When coordinates are fully excluded:
head(SSP.58$Coordinates)

#When dummy coordinates are provided, to provide the spatial structure of the data but anonymize the real locations:
head(UL.120$Coordinates)
```

The coordinates are arranged in the same order as the tabular dataset. This allows them to be merged with the dataset using a simple `cbind()`:

```{r}
BB.250_dataset.with.coordinates <- cbind(BB.250$Coordinates,BB.250$Dataset) # Merge coordinates with dataset

library(sf) # For visualizing spatial data 
BB.250_dataset.with.coordinates.sf <- st_as_sf(x = BB.250_dataset.with.coordinates, 
                         coords = c("x_25833", "y_25833")) # Make it spatial
st_crs(BB.250_dataset.with.coordinates.sf) <- 25833 

plot(BB.250_dataset.with.coordinates.sf[c(1:3)])
```

### Modelling with Lime.SoDa

To ensure comparability between different studies using Lime.SoDa, the pre-defined folds should be used for validation purposes. These folds were created by randomly partitioning of the dataset into 10 equally sized folds, allowing for a 10-fold cross-validation. The training and testing data can be indexed through the `$Folds` vector:

```{r}
BB.250$Folds # Folds are defined through a vector with numbers from 1 - 10 with 1/10 of the data in each fold

# Splitting the dataset into training and testing folds for the example of the first fold
training_data_BB.250 <- BB.250$Dataset[BB.250$Folds != 1,] 
nrow(training_data_BB.250) # Show size of training 9-fold 
testing_data_BB.250 <- BB.250$Dataset[BB.250$Folds == 1,]
nrow(testing_data_BB.250) # Show size of validation fold
```

Although not specifically defined for this purpose, the same folds can also be used for a simple train-test split instead of cross-validation. For example, the first 9 folds can be used as training data and the 10th fold as testing data for a 90/10 train-test split. Similarly, one can use 8 folds for training and 2 folds for testing to achieve an 80/20 split, and so on. This also allows comparability when using single-split validation:

```{r}
# Splitting the dataset into training and testing sets for a 70:30 split:
training_data_BB.250 <- BB.250$Dataset[!(BB.250$Folds %in% c(8:10)),]
nrow(training_data_BB.250)
testing_data_BB.250 <- BB.250$Dataset[(BB.250$Folds %in% c(8:10)),]
nrow(testing_data_BB.250)
```

In the following  example, the performance of a multiple linear regression for `BB.250` is evaluated for SOC predictions by looping over the folds (i.e. 10-fold cross validation):

```{r}
set.seed(2025)
BB.250_dataset_predict.SOC <- BB.250$Dataset[-c(2:3)] # In this example, only SOC is predicted. Hence, Clay_target and pH_target are excluded

predicted_values <- c() # To store predicted values in this vector
testing_values <- c() # To store testing fold values in this vector

for (i in 1:10){
  training_data <- BB.250_dataset_predict.SOC[BB.250$Folds != i,] # Exclude testing fold from training data
  testing_data <- BB.250_dataset_predict.SOC[BB.250$Folds == i,] # Define testing fold as testing data
  
  MLR_model <- lm("SOC_target~.", data = training_data) # Fit MLR to predict SOC
  MLR_model_predictions <- predict(MLR_model, testing_data) # Predict values of testing fold
  
  predicted_values <- c(predicted_values, MLR_model_predictions) # Store predicted values for testing fold
  testing_values <- c(testing_values, testing_data$SOC_target) # Store real observed values from testing fold
}

# Since we aggregated the predictions and testing values, we can now calculate the R-squared and RMSE for the model performance with MLR:
# R-squared
1 - sum((predicted_values - testing_values)^2) / sum((testing_values - mean(testing_values))^2)
# RMSE
sqrt(mean((predicted_values - testing_values)^2))
```

In this simple example, an R-squared of 0.75 and an RMSE of 0.24 were achieved. Using the same pre-defined folds, it is now possible to evaluate how other learning algorithms perform on the same dataset in comparison to MLR, without the necessity to rerun the code. This is particularly useful when comparing more complex algorithms or methods that are difficult to reproduce.

### How to cite Lime.SoDa

Please cite the associated paper Schmidinger et al. (2025) when using data from Lime.SoDa. We also refer to this paper for more information about the context of Lime.SoDa:

```{r}
citation("Lime.SoDa")
```


